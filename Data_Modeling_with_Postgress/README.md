# Data Modeling with Postgres


## Table Of Contents:

* [Overview](#overview)
* [Purpose](#purpose)
* [Datasets](#data)
    * [Songs Dataset](#songs)
    * [Log Dataset](#logs)
* [Schema](#schema)
    * [Fact Table](#fact)
    * [Dimension Tables](#dim)
* [Project Files](#files)
* [Environment](#env)
* [How To Run](#run)
* [Reference](#ref)


<a id='intro'></a>

## Overview

From the Udacity Project Description:
> In this project, you'll apply what you've learned on data modeling with Postgres and build an ETL pipeline using Python. To complete the project, you will need to define fact and dimension tables for a star schema for a particular analytic focus, and write an ETL pipeline that transfers data from files in two local directories into these tables in Postgres using Python and SQL.


<a id='purpose'></a>

## Purpose

The purpose of this database is to allow the song streaming service to have their data in a more accessable format than JSON files, for running queries and analysis. Just what analysis is never stated, but the architecture of the database should make most common analytical tasks possible.


<a id='data'></a>

## Datasets

Both datasets are provided to the student in a multi-level directory structure containing multiple JSON files. It seems unfortunate that so few songs and artists that are included in the logs are also included in the songs data.


<a id='songs'></a>

### Songs Dataset

The song data is a subset of the [Million Song Dataset](http://millionsongdataset.com/).

Sample Record:
```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```


<a id='logs'></a>

### Log Dataset

The log data was generated by [Event Simulator](https://github.com/Interana/eventsim).

Sample Record:
```
{"artist": null, "auth": "Logged In", "firstName": "Walter", "gender": "M", "itemInSession": 0, "lastName": "Frye", "length": null, "level": "free", "location": "San Francisco-Oakland-Hayward, CA", "method": "GET","page": "Home", "registration": 1540919166796.0, "sessionId": 38, "song": null, "status": 200, "ts": 1541105830796, "userAgent": "\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"", "userId": "39"}
```


<a id='schema'></a>

## Schema

A star schema is to be used, as specified in the project description from Udacity (see "Overview" above.) This schema will allow the data to be structured in a logical way with one Fact table and a number of Dimension tables to fill in additional information for the fact, as necessary.

Apropriate keys and contstraints to mamtain referential integrity are included for all tables.


<a id='fact'></a>

### Fact Table

**`songplays`** - holds data for songs played (records with `"page": "NextSong"`.) This data is extracted from the log dataset JSON files and transformed before being inserted. The timestamp data is transformed most, while the other values are generally inserted as they come from the source.

It would not be unusual for names to be broken into first and last instead of being stored as a single value, this would make customer contact easier by allowing less formal addressing of users, but then it could be that there is another database for customers and billing and such.

Columns:
```
songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
```


<a id='dim'></a>

### Dimension Tables

**`users`**  - users of the app from Event Simulator log data
Columns:
```
user_id, first_name, last_name, gender, level
Columns:
```
**`songs`**  - songs from the subset of the Million Song Database as provided by Udacity.
Columns:
```
song_id, title, artist_id, year, duration
```
**`artists`**  - artists from the subset of the Million Song Database as provided by Udacity.
Columns:
```
artist_id, name, location, latitude, longitude
```
**`time`**  - timestamps of events in  Event Simulator data; broken down into specific units
Columns:
```
start_time, hour, day, week, month, year, weekday
```


<a id='files'></a>

## Project Files

```create_tables.py``` -> A python file with a function drop and create the database, and functions to call the SQL templates to drop and create the tables.

```etl.ipynb``` -> A jupyter notebook to work out the *Extract, Transform and Load (ETL)* process. 

```etl.py``` -> A python file to run the ETL on **`song_data`** and **`log_data`**

```main.py``` -> A python file to make running the whole project a simple single command. See [How to run](#run) below.

```sql_queries.py``` -> A python file with SQL for DROP and CREATE queries for the fact and dimension tables as well as SQL INSERT templates for adding new records into all the tables, and a SELECT statement template for finding song data.

```test.ipynb``` -> A jupyter notebook to connect to postgres db and validate database creation and ETL of the data.

```README.md``` -> this file


<a id='env'></a>

## Environment 

Python 3.6 or above

PostgresSQL 9.5 or above

psycopg2 - PostgreSQL database adapter for Python


<a id='run'></a>

## How to run

Run the **`main`** program ```main.py```:
```
python main.py
``` 

```create_tables.py``` and ```etl.py``` file may be run independently:
```
python create_tables.py 
python etl.py 
```

<a id='ref'></a>

#### Reference:

[Pandas Documentation](https://pandas.pydata.org/pandas-docs/stable/)

[PostgreSQL Documentation](https://www.postgresql.org/docs/)

[Psycopg](http://initd.org/psycopg/docs/)

[Python Documentation](https://docs.python.org/3/)

[Udacity](https://www.udacity.com/)
